std::vector<Point> SPME::calcReducedCoords(const std::vector<Point>& localRealCoordinates,
                                           const MDSystem& system)
{
  std::vector<Point> localReducedCoords;
  size_t idx;
  Point coord;  // Fractional coordinates; 3 - vector

  // bool Orthorhombic; true if simulation cell is orthorhombic, false if it's generic
  size_t numParticles = localRealCoordinates.size();
  Matrix3 inverseBox = system.getCellInverse();        // For generic coordinate systems
  if (!system.isOrthorhombic()) {
    for (idx = 0; idx < numParticles; ++idx) {
      coord = localRealCoordinates[idx];                   // Get non-ghost particle coordinates for this cell
      coord = (inverseBox * coord.asVector()).asPoint();   // InverseBox is a 3x3 matrix so this is a matrix multiplication = slow
      localReducedCoords.push_back(coord);                 // Reduced non-ghost particle coordinates for this cell
    }
  } else {
    for (idx = 0; idx < numParticles; ++idx) {
      coord = localRealCoordinates[idx];        // Get non-ghost particle coordinates for this cell
      coord(0) *= inverseBox(0, 0);
      coord(1) *= inverseBox(1, 1);
      coord(2) *= inverseBox(2, 2);               // 6 Less multiplications and additions than generic above
      localReducedCoords.push_back(coord);      // Reduced, non-ghost particle coordinates for this cell
    }
  }
  return localReducedCoords;
}

---------------------------------------------------------------------------------------------------

    //    friend std::ostream& operator<<(std::ostream& out,
    //                                    const Uintah::SimpleGrid<T>& sg);
    
---------------------------------------------------------------------------------------------------

// Old code from monolithic SPME::Calculate()
bool converged = false;
int numIterations = 0;
int maxIterations = d_system->getMaxIterations();
while (!converged && (numIterations < maxIterations)) {

  std::vector<SPMEPatch*>::iterator iter;
  for (iter = d_spmePatches.begin(); iter != d_spmePatches.end(); iter++) {
    SPMEPatch* spmePatch = *iter;

    const Patch* patch = spmePatch->getPatch();
    ParticleSubset* pset = old_dw->getParticleSubset(materials->get(0), patch);

    constParticleVariable<Point> px;
    constParticleVariable<long64> pids;
    constParticleVariable<double> pcharge;
    old_dw->get(px, d_lb->pXLabel, pset);
    old_dw->get(pids, d_lb->pParticleIDLabel, pset);
    old_dw->get(pcharge, d_lb->pChargeLabel, pset);

    std::vector<SPMEMapPoint> gridMap = generateChargeMap(pset, px, pids, d_interpolatingSpline);
    mapChargeToGrid(spmePatch, gridMap, pset, pcharge, d_interpolatingSpline.getHalfMaxSupport());  // Calculate Q(r)

    SimpleGrid<complex<double> > Q = spmePatch->getQ();
    SimpleGrid<double> fTheta = spmePatch->getTheta();
    Q.initialize(complex<double>(0.0, 0.0));
    SimpleGrid<Matrix3> stressPrefactor = spmePatch->getStressPrefactor();

    // Map the local patch's charge grid into the global grid and transform
    //      SPME::GlobalMPIReduceChargeGrid(Ghost::AroundNodes);  //Ghost points should get transferred here
    //      SPME::ForwardTransformGlobalChargeGrid();  // Q(r) -> Q*(k)

    // Once reduced and transformed, we need the local grid re-populated with Q*(k)
    //      SPME::MPIDistributeLocalChargeGrid(Ghost::None);

    // Multiply the transformed Q out
    IntVector localExtents = spmePatch->getLocalExtents();
    size_t xExtent = localExtents.x();
    size_t yExtent = localExtents.y();
    size_t zExtent = localExtents.z();
    double localEnergy = 0.0;//Maybe should be global?
    Matrix3 localStress(0.0);//Maybe should be global?
    for (size_t kX = 0; kX < xExtent; ++kX) {
      for (size_t kY = 0; kY < yExtent; ++kY) {
        for (size_t kZ = 0; kZ < zExtent; ++kZ) {
          SimpleGrid<double> fTheta = spmePatch->getTheta();
          complex<double> gridValue = Q(kX, kY, kZ);

          // Calculate (Q*Q^)*(B*C)
          Q(kX, kY, kZ) = gridValue * conj(gridValue) * fTheta(kX, kY, kZ);
          localEnergy += std::abs(Q(kX, kY, kZ));
          localStress += std::abs(Q(kX, kY, kZ)) * stressPrefactor(kX, kY, kZ);
        }
      }
    }

    // Transform back to real space
    //      SPME::GlobalMPIReduceChargeGrid(Ghost::None);  //Ghost points should NOT get transferred here
    //      SPME::ReverseTransformGlobalChargeGrid();
    //      SPME::MPIDistributeLocalChargeGrid(Ghost::AroundNodes);

    //  This may need to be before we transform the charge grid back to real space if we can calculate
    //    polarizability from the fourier space component
    converged = true;
    if (d_polarizable) {
      // calculate polarization here
      // if (RMSPolarizationDifference > PolarizationTolerance) { ElectrostaticsConverged = false; }
      std::cerr << "Error:  Polarization not currently implemented!";
    }
    // Sanity check - Limit maximum number of polarization iterations we try
    ++numIterations;
  }
  //    SPME::GlobalReduceEnergy();
  //    SPME::GlobalReduceStress();  //Uintah framework?
}




#if !defined(__digital__) || defined(__GNUC__)
  template<>
#endif
  void ReductionVariable<LinearArray3<dblcomplex>, Reductions::Sum<LinearArray3<dblcomplex> > >::getMPIData(vector<char>& data,
                                                                                                            int& index)
  {
    int count = value.dim1() * value.dim2() * value.dim3();
    ASSERTRANGE(index, 0, static_cast<int>( (data.size() + 1) - (count * sizeof(std::complex<double>)) ));

    std::complex<double>* ptr = reinterpret_cast<dblcomplex*>(&data[index]);
    long int size = value.getSize();
    for (long idx = 0; idx < size; ++idx) {
      *ptr++ = value.get_dataptr()[idx];
    }
  }

#if !defined(__digital__) || defined(__GNUC__)
  template<>
#endif
  void ReductionVariable<LinearArray3<dblcomplex>, Reductions::Sum<LinearArray3<dblcomplex> > >::putMPIData(vector<char>& data,
                                                                                                            int& index)
  {
    int count = value.dim1() * value.dim2() * value.dim3();
    ASSERTRANGE(index, 0, static_cast<int>( (data.size() + 1) - (count * sizeof(std::complex<double>)) ));

    std::complex<double>* ptr = reinterpret_cast<dblcomplex*>(&data[index]);
    long size = value.getSize();
    for (long idx = 0; idx < size; ++idx) {
      value.get_dataptr()[idx] = *ptr++;
    }
  }

#if !defined(__digital__) || defined(__GNUC__)
  template<>
#endif
  void ReductionVariable<LinearArray3<dblcomplex>, Reductions::Sum<LinearArray3<dblcomplex> > >::getMPIInfo(int& count,
                                                                                                            MPI_Datatype& datatype,
                                                                                                            MPI_Op& op)
  {
    datatype = MPI_C_DOUBLE_COMPLEX;
    count = value.getSize();
    op = MPI_SUM;
  }
  
---------------------------------------------------------------------------------------------------  
  
task->setType(Task::OncePerProc);
LoadBalancer* loadBal = sched->getLoadBalancer();
GridP grid = level->getGrid();
const PatchSet* perprocPatches = loadBal->getPerProcessorPatchSet(grid);
sched->addTask(task, perprocPatches, d_sharedState->allMaterials());

---------------------------------------------------------------------------------------------------


